{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ac2ba6-2111-44ed-bb57-488e846f3c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL:  https://www.google.co.in/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not allowed to scrape\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import urllib.robotparser\n",
    "import urllib.parse\n",
    "\n",
    "# 1. Extract Specific Information (Headlines, Links, Images, etc.)\n",
    "def extract_information(url):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # Extract headlines\n",
    "        headlines = []\n",
    "        for i in range(1, 7):\n",
    "            headlines.extend([h.get_text() for h in soup.find_all(f'h{i}')])\n",
    "        \n",
    "        # Extract links\n",
    "        links = [(link.get('href'), link.get_text()) for link in soup.find_all('a') if link.get('href')]\n",
    "        \n",
    "        # Extract images\n",
    "        images = [(img.get('src'), img.get('alt')) for img in soup.find_all('img')]\n",
    "        \n",
    "        return headlines, links, images\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return None, None, None\n",
    "\n",
    "# 2. Save Data (CSV, JSON)\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 3. Handle Errors and Retry\n",
    "def make_request(url, retries=3):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Error: {err}\")\n",
    "        if retries > 0:\n",
    "            time.sleep(2)\n",
    "            return make_request(url, retries - 1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# 4. Extract All Links\n",
    "def extract_all_links(url):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        links = [(link.get('href'), link.get_text()) for link in soup.find_all('a') if link.get('href')]\n",
    "        return links\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return []\n",
    "\n",
    "# 5. Search for Specific Text\n",
    "def search_for_text(url, keyword):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        occurrences = soup.body.find_all(string=lambda text: keyword.lower() in text.lower())\n",
    "        return occurrences\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return []\n",
    "\n",
    "# 6. Use CSS Selectors\n",
    "def use_css_selectors(url, selector):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        elements = soup.select(selector)\n",
    "        return elements\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return []\n",
    "\n",
    "# 7. Count Elements\n",
    "def count_elements(url, tag):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        elements = soup.find_all(tag)\n",
    "        return len(elements)\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return 0\n",
    "\n",
    "# 8. Follow Links (Recursive Scraping)\n",
    "def follow_links(url, depth=1):\n",
    "    if depth < 0:\n",
    "        return []\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        links = [link.get('href') for link in soup.find_all('a') if link.get('href') and link.get('href').startswith('/wiki')]\n",
    "        return links\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return []\n",
    "\n",
    "# 9. Respect Robots.txt\n",
    "def can_fetch(url, user_agent='*'):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(base_url)\n",
    "    rp.read()\n",
    "    return rp.can_fetch(user_agent, url)\n",
    "\n",
    "# Get URL from user input\n",
    "url = input(\"Enter URL: \")\n",
    "\n",
    "# Example usage of the functions\n",
    "if can_fetch(url):\n",
    "    print(\"Allowed to scrape\")\n",
    "    \n",
    "    # Extract information\n",
    "    headlines, links, images = extract_information(url)\n",
    "    print(\"Headlines:\", headlines)\n",
    "    print(\"Links:\", links)\n",
    "    print(\"Images:\", images)\n",
    "\n",
    "    # Save to CSV and JSON\n",
    "    save_to_csv(links, 'links.csv')\n",
    "    save_to_json(images, 'images.json')\n",
    "\n",
    "    # Make request with retries\n",
    "    page = make_request(url)\n",
    "    if page:\n",
    "        print(\"Request successful\")\n",
    "    else:\n",
    "        print(\"Request failed after retries\")\n",
    "\n",
    "    # Extract all links\n",
    "    all_links = extract_all_links(url)\n",
    "    print(\"All Links:\", all_links)\n",
    "\n",
    "    # Search for specific text\n",
    "    keyword = input(\"Enter keyword to search: \")\n",
    "    results = search_for_text(url, keyword)\n",
    "    print(f\"Occurrences of '{keyword}':\", results)\n",
    "\n",
    "    # Use CSS selectors\n",
    "    selector = input(\"Enter CSS selector to use: \")\n",
    "    elements = use_css_selectors(url, selector)\n",
    "    print(f\"Elements matching '{selector}':\", elements)\n",
    "\n",
    "    # Count elements\n",
    "    tag = input(\"Enter HTML tag to count: \")\n",
    "    count = count_elements(url, tag)\n",
    "    print(f\"Number of '{tag}' elements:\", count)\n",
    "\n",
    "    # Follow links\n",
    "    depth = int(input(\"Enter depth to follow links: \"))\n",
    "    followed_links = follow_links(url, depth)\n",
    "    print(\"Followed Links:\", followed_links)\n",
    "else:\n",
    "    print(\"Not allowed to scrape\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c80e8-ca0e-4424-8b6d-fbbfaeee01dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
