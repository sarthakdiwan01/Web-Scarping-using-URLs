{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7896286e-71ed-4ecd-baf9-5c7a509744fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import urllib.robotparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "906fdb91-f30d-43f7-8c6e-0f90b5e457ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract Specific Information (Headlines, Links, Images, etc.)\n",
    "def extract_information(url):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # Extract headlines\n",
    "        headlines = []\n",
    "        for i in range(1, 7):\n",
    "            headlines.extend([h.get_text() for h in soup.find_all(f'h{i}')])\n",
    "        \n",
    "        # Extract links\n",
    "        links = [(link.get('href'), link.get_text()) for link in soup.find_all('a') if link.get('href')]\n",
    "        \n",
    "        # Extract images\n",
    "        images = [(img.get('src'), img.get('alt')) for img in soup.find_all('img')]\n",
    "        \n",
    "        return headlines, links, images\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7420bdb-468a-4ed6-a3b4-bcd559b804e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Save Data (CSV, JSON)\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa67a64-6c49-44e2-a8be-fa53afec121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Handle Errors and Retry\n",
    "def make_request(url, retries=3):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Error: {err}\")\n",
    "        if retries > 0:\n",
    "            time.sleep(2)\n",
    "            return make_request(url, retries - 1)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5cc2aa0-6ac6-4fb3-bae9-5d2a9bfe1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract All Links\n",
    "def extract_all_links(url):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        links = [(link.get('href'), link.get_text()) for link in soup.find_all('a') if link.get('href')]\n",
    "        return links\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98ee89c9-d385-45ab-b085-c16fea1c1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Search for Specific Text\n",
    "def search_for_text(url, keyword):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        occurrences = soup.body.find_all(string=lambda text: keyword.lower() in text.lower())\n",
    "        return occurrences\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808b185a-f23b-4c33-ab38-f5b642011b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Use CSS Selectors\n",
    "def use_css_selectors(url, selector):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        elements = soup.select(selector)\n",
    "        return elements\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "267a255f-0a32-43c3-89ca-5b139f4a7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Count Elements\n",
    "def count_elements(url, tag):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        elements = soup.find_all(tag)\n",
    "        return len(elements)\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "483f5546-4dfe-4e8b-942b-4fa0ce823112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Follow Links (Recursive Scraping)\n",
    "def follow_links(url, depth=1):\n",
    "    if depth < 0:\n",
    "        return []\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        links = [link.get('href') for link in soup.find_all('a') if link.get('href') and link.get('href').startswith('/wiki')]\n",
    "        return links\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {page.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c2f372-b61a-4f10-8cb7-c78dee1f9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Respect Robots.txt\n",
    "def can_fetch(url, user_agent='*'):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(base_url)\n",
    "    rp.read()\n",
    "    return rp.can_fetch(user_agent, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "187424d0-49f7-4b17-bd79-eb55e774f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL:  www.google.com\n"
     ]
    }
   ],
   "source": [
    "# Get URL from user input\n",
    "url = input(\"Enter URL: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3aed392-91f6-4818-9823-c91969351a93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown url type: ':///robots.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage of the functions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcan_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllowed to scrape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Extract information\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m, in \u001b[0;36mcan_fetch\u001b[1;34m(url, user_agent)\u001b[0m\n\u001b[0;32m      5\u001b[0m rp \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrobotparser\u001b[38;5;241m.\u001b[39mRobotFileParser()\n\u001b[0;32m      6\u001b[0m rp\u001b[38;5;241m.\u001b[39mset_url(base_url)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rp\u001b[38;5;241m.\u001b[39mcan_fetch(user_agent, url)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\robotparser.py:62\u001b[0m, in \u001b[0;36mRobotFileParser.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reads the robots.txt URL and feeds it to the parser.\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m401\u001b[39m, \u001b[38;5;241m403\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:503\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, fullurl, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39msocket\u001b[38;5;241m.\u001b[39m_GLOBAL_DEFAULT_TIMEOUT):\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# accept a URL or a Request object\u001b[39;00m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fullurl, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 503\u001b[0m         req \u001b[38;5;241m=\u001b[39m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullurl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    505\u001b[0m         req \u001b[38;5;241m=\u001b[39m fullurl\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:322\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[1;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    320\u001b[0m              origin_req_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, unverifiable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    321\u001b[0m              method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_url\u001b[49m \u001b[38;5;241m=\u001b[39m url\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munredirected_hdrs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:348\u001b[0m, in \u001b[0;36mRequest.full_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url \u001b[38;5;241m=\u001b[39m unwrap(url)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfragment \u001b[38;5;241m=\u001b[39m _splittag(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url)\n\u001b[1;32m--> 348\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:377\u001b[0m, in \u001b[0;36mRequest._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype, rest \u001b[38;5;241m=\u001b[39m _splittype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown url type: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_url)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselector \u001b[38;5;241m=\u001b[39m _splithost(rest)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost:\n",
      "\u001b[1;31mValueError\u001b[0m: unknown url type: ':///robots.txt'"
     ]
    }
   ],
   "source": [
    "# Example usage of the functions\n",
    "if can_fetch(url):\n",
    "    print(\"Allowed to scrape\")\n",
    "    \n",
    "    # Extract information\n",
    "    headlines, links, images = extract_information(url)\n",
    "    print(\"Headlines:\", headlines)\n",
    "    print(\"Links:\", links)\n",
    "    print(\"Images:\", images)\n",
    "\n",
    "    # Save to CSV and JSON\n",
    "    save_to_csv(links, 'links.csv')\n",
    "    save_to_json(images, 'images.json')\n",
    "\n",
    "    # Make request with retries\n",
    "    page = make_request(url)\n",
    "    if page:\n",
    "        print(\"Request successful\")\n",
    "    else:\n",
    "        print(\"Request failed after retries\")\n",
    "\n",
    "    # Extract all links\n",
    "    all_links = extract_all_links(url)\n",
    "    print(\"All Links:\", all_links)\n",
    "\n",
    "    # Search for specific text\n",
    "    keyword = input(\"Enter keyword to search: \")\n",
    "    results = search_for_text(url, keyword)\n",
    "    print(f\"Occurrences of '{keyword}':\", results)\n",
    "\n",
    "    # Use CSS selectors\n",
    "    selector = input(\"Enter CSS selector to use: \")\n",
    "    elements = use_css_selectors(url, selector)\n",
    "    print(f\"Elements matching '{selector}':\", elements)\n",
    "\n",
    "    # Count elements\n",
    "    tag = input(\"Enter HTML tag to count: \")\n",
    "    count = count_elements(url, tag)\n",
    "    print(f\"Number of '{tag}' elements:\", count)\n",
    "\n",
    "    # Follow links\n",
    "    depth = int(input(\"Enter depth to follow links: \"))\n",
    "    followed_links = follow_links(url, depth)\n",
    "    print(\"Followed Links:\", followed_links)\n",
    "else:\n",
    "    print(\"Not allowed to scrape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a1192-4b5b-4928-ac46-3d75aa6d06aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
